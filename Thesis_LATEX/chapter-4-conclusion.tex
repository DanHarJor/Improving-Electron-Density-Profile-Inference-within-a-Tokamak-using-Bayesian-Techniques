\chapter{Conclusion}

The tokamak is the most researched device that has the highest chance of becoming a viable fusion reactor.  Within the tokamak, it is vital to be able to measure the electron density throughout the device to ensure safety limits are maintained and fusion performance goals are met. The equilibrium of the tokamak plasma has toroidally symmetric flux surfaces on which the electron density is constant. The flux surfaces allow a 1D electron density profile to express the density throughout the tokamak. The flux surfaces are provided by the \gls{nice} code which uses magnetic diagnostic data to determine the flux surfaces. There are multiple diagnostics that can measure the electron density. This thesis focused on the interferometry diagnostic within the \gls{west} tokamak. Interferometry is a technique that uses the optical path length difference of vacuum to plasma for multiple lasers fired through the plasma to determine the line integrated electron density along the laser's line of sight. This can provide enough information to infer the electron density profile with Bayesian techniques. Both real and synthetic data were used to test various Bayesian techniques. The various techniques were different ways to deal with the hyperparameters that defined the prior. Three different kernels were trialled that defined the functional form of the smoothness of the final inferred profile. The parameters in the kernels were decided using a \gls{map} of the hyperparameter posterior or with a full Bayesian analysis which involves marginalising the hyperparameters by sampling from a joint probability distribution of the electron density profiles posterior and the hyperparameter posterior. Once the hyperparameters are known then the posterior of the density profile can be computed analytically; although this process requires matrix inversion. The prior information also included that we know the gradient is close to 0 in the core and the density is close to 0 at the last closed flux surface. This was included in the Bayesian analysis using artificial observations. \gls{nice} also provides an electron density profile for real \gls{west} shots and can be compared to the Bayesian methods.

Five synthetic profiles were defined and used to create synthetic interferometry data. These included L-mode, H-mode, Peaked, L-mode Peaked and H-mode Peaked. They are all scientifically relevant profile shapes. Real interferometry data from a typical \gls{west} shot was also used. The inferred profiles of the synthetic data could be compared to the synthetic profiles and the inferred profiles from the real data could be compared to the \gls{nice} profile. The full Bayesian approach involves \gls{mcmc} sampling which was carried out using the emcee python package. The emcee parameters are turned to minimise the autocorrelation. There was still substantial autocorrelation that reduced the reliability of the samples to be from the true hyperparameter posterior distribution. This reduces the credibility of this implementation of the full Bayesian approach. The final inferences show potential for the methods explored but are inconclusive as to which is the best method. There was no combination of the kernels and hyperparameter method that was able to perform well on all of the profiles of interest. A few of the inferences very well match the synthetic or \gls{nice} profile; although for a real scenario, there is no comparison profile to assess performance and the inference algorithm must be able to cope with any ground truth profile.

For future investigation, more optimisation techniques could be deployed for finding the \gls{map} of the hyperparameter posterior of the various kernels. The cubic spline function for the hyperbolic tangent was the most flexible function used which should allow it to be the most effective for a wide variety of profiles, the number of spline knots and their prior positions could be better tuned to improve performance. The amplitude hyperparameter determines how far from the prior profile the inference can easily go. This is an intuitive parameter that could potentially be manually selected to reduce the dimensionality of the parameter space to be searched and increase the chance an optimal length-scale function could be selected. \gls{west} also collects polarimetry data and this also contains information about the electron density profile. More work could be done to include this information in the Bayesian inference. This would involve using the temperature profile to get the current profile which could be used to get the poloidal magnetic field which could then separate the electron density from the poloidal magnetic field in the polarimetry phase shift shown in equation, \ref{eq:pol_farad}. There is also information available about the amount of fuel injected into the system and this is correlated to the electron density. This could be used to get a more accurate prior for the electron density profile. Once the algorithm is stable and consistently provides accurate inferences then the next step would be to test the real-time capabilities. The slowest process is the hyperparameter optimisation and after that the matrix inversion. Perhaps these could be replaced with reduced models or neural networks to ensure real-time computation speeds.

