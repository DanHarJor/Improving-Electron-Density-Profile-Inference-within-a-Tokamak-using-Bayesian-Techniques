\chapter{Background Theory of Bayesian Techniques and WEST Interferometry}

This chapter aims to equip the reader with necessary background theory required to reproduce this work and to understand the origin of the inferred electron density profiles presented in the results. It first describes a tokamak fusion device and some relevant physics concepts behind its function. It then describes in a high level manner the inference carried out by Blaise Faugeras and team with their code known as \gls{nice} \cite{nice}. After the chapter outlines Bayesian inference and it's connection with \gls{gpr}. Then the \gls{gpr} method is explained in detail for a simple regression problem. Interferometry is introduced in enough detail that the \gls{gpr} method can be altered to infer the electron density profile. The key factor being the new responce matrix. There are various options for advanced alterations that are explained here and explored in the results section.

\section{The Tokamak}

\begin{figure}[H]
  \centering
  \includegraphics[width=10cm]{images/tokamak.jpg}
  \caption{A tokamak and relevant magnetic fields that create the helical particle trajectory \cite{tokamakSchema}.}
  \label{fig:tokamakSchema}
\end{figure}

Tokamak is a class of fusion device whose name comes from the abbreviation of a Russian phrase which means ``toroidal chamber with magnetic coils''. It consists of a doughnut shaped vacuum chamber surrounded by powerful magnets that aim to confine a high temperature plasma that would otherwise vaporise the chamber. The plasma pressure and temperature are fundamental parameters in the context of nuclear fusion because they dictate the conditions required to overcome the electrostatic repulsion between positively charged atomic nuclei and bring them close enough for the strong nuclear force to initiate fusion reactions. In the core of stars like our Sun, the immense pressure and temperature generated by the gravitational collapse create the conditions where hydrogen nuclei (protons) can overcome their natural repulsion and fuse into helium, releasing a tremendous amount of energy in the process. To initiate fusion, hydrogen must be heated to temperatures in the range of tens of millions of degrees Celsius. In a tokamak, this is mainly accomplished with ohmic heating via a driving plasma current and neutral gas injection. This involves accelerating hydrogen ions to high speeds with electric fields and neutralising them the instant before they enter the chamber. The resulting plasma attains the required temperature, allowing nuclei to collide with sufficient energy for fusion reactions to occur. Figure \ref{fig:tokamakSchema} shows the position of various magnetic field coils within the tokamak. The toroidal magnetic field exerts an inward force on the plasma thus raising its pressure. A high pressure is required to increase the frequency of collisions so that the energy output could exceed the large heating energy input. The central solenoid induces a current in the plasma which produces the majority of the poloidal magnetic field. This field is essential for confinement but it also plays a key role in plasma stability. The outer poloidal field coils can be controlled in real time to help mitigate instabilities. A real time inference of the electron density profile would assist in identifying instabilities and informing the algorithm that drives the control coils to mitigate them. In addition to high temperature and pressure, the tokamak design seeks to maximize the confinement time of the plasma. This is essential to allow a sufficient number of fusion reactions to occur before the plasma cools down or loses its stability. The magnetic fields in a tokamak are carefully optimized to prevent rapid plasma loss and minimize heat loss through various mechanisms, including turbulent transport. The shape of the density profile has a large effect on the confinement time.

The combination of the toroidal and poloidal fields shown in figure \ref{fig:tokamakSchema} create a helical magnetic field within the plasma. Electrons and ions are accelerated in opposite toroidal directions by the central solenoid yet the both follow a trajectory along the magnetic field lines. This is because a charged particle moving across a magnetic field is succumb to a force perpendicular to its motion. This causes them to gyrate around the magnetic field lines and confines them to follow the magnetic field lines. This is an over simplification although a detailed description of particle motion within a magnetic field is not needed for the purpose of this thesis. It is enough to know that if a single charged particle was within a tokamak then it would perfectly follow a trajectory along the helical path of the magnetic field lines with a small gyration around the field line. When many particles are introduced then collisions can interrupt these trajectories, yet in many models used for data analysis the assumption that particles follow the magnetic field lines is used, including within this thesis. 

The magnetic field lines are confined to magnetic flux surfaces, figure \ref{fig:magfluxsurf}. The toroidal and poloidal flux is constant on magnetic flux surfaces, there is 0 flux across magnetic flux surfaces. Since we assume that the particles follow the magnetic field lines which are strictly bound to these surfaces, we also assume that the density is constant on these surfaces. This allows the density of the entire cross-section to be expressed with a 1D profile as a function of normalised radius $\rho$, figure \ref{fig:nice_example}. Where $\rho$ is 0 at the plasma core and 1 at the edge. At the core the poloial magnetic flux is maximum and the edge is the last closed flux surface. Particles past the edge are no longer bound and may interact with the plasma wall. The existance of nested magnetic flux surfaces shown in figure \ref{fig:magfluxsurf} rely on the ideal \gls{mhd} assumptions. Experiments frequently discover magnetic islands which discredits the assumption of nested flux surfaces. The electron density profiles infered rely on this assumption and for many applications, such as real time control a highly accurate infernce is often not required.

\begin{figure}
  \centering
  \includegraphics[width=10cm]{images/fluxsurf.png}
  \caption{Magnetic flux surfaces \cite{wessontokamak}.}
  \label{fig:magfluxsurf}
\end{figure}

\section{NICE}

\gls{nice} is an equilibrium reconstruction code developed for the \gls{west} tokamak. It is relevant because it computes an inference of the electron density profile that is available for comparison to the profile inferred in this work, although \glspl{nice} main objective is to infer the shape and position of the magnetic flux surfaces. \gls{nice} uses magnetic diagnostics. At WEST these include 421 pickup coils, 36 flux loops and 12 Rogowski coils \cite{westmagdiag}. The magnetic diagnostics provide the majority of the information. \gls{nice} also uses interferometry, polarimetry, motional stark effect and pressure measurements. Equation \ref{eq:int_phase} and \ref{eq:pol_farad} further in the chapter, show how interferometry and polarimetry together can provide information about the poloidal magnetic field, which directly affects the magnetic flux and thus magnetic flux surfaces. \gls{nice} performs the inference by minimising a cost function. The cost function determines how well a physical state of the system matches the data received. A state being a specific position and shape of the magnetic flux surfaces and electron density profile. This requires a forward model. The forward model takes a state of the system attempts to compute the signals that would be received by error free diagnostics, if that state was the ground truth. The forward model is a simplified mathematical representation of the measurement process and can never be 100\% accurate. This introduces errors into the inference that need to be accounted for. The signals from the forward model can be compared to the actual signals received by the diagnostics in order to compute the cost function. By minimising the cost function the state that best matches the data is found. \gls{nice} uses \gls{sqp} as the minimisation algorithm. The optimal state of the system is then stored in the \gls{imas} database for \gls{west}. This includes the 1D electron density profile used as a comparison for the profile inferred in this work. \gls{nice} also imposes regularisation terms on their cost function. These penalise the cost function when state properties have features that disagree with prior knowledge. This includes smoothness. We expect the magnetic flux surfaces and electron density profile, to be continuous and smooth. A state inputted into the cost function that is not smooth triggers the regularisation term which causes the cost function to be larger. Minimising the cost function now also leads to smooth magnetic flux surfaces and electron density profile. This leads into a difficult question, how smooth should it be? They also have a regularisation term to penalise the cost function if the electron density profile is far from 0 at the last closed flux surface or plasma edge. It is prior knowledge that the electron density is near 0 at the plasma edge. How close to 0, and how strong should the regularisation be is still an open question. This work's \gls{gpr} approach has  direct analogs to these regularisation terms. As explained later in more detail the length-scale controls smoothness and an artifical observation ensures the density is close to zero at the edge. Figure \ref{fig:nice_example} shows an example of a \gls{nice} inferred electron density profile. It is modeled with a cubic spline function. It is the parameters of the cubic spline that are inputted into the cost function. The errors are calculated using a sensitivity method. In short the error is deemed larger for the electron density of a particular normalised radius if a large change in the density leads to a small change in the cost function. In this case we cannot be certain what density is more true because many lead to a similarly low cost function and thus match the data similarly well. To include some more details, the \gls{sqp} minimisation algorithm computes the hessian of the cost function for minimisation, but this hessian can also be used to measure the sensitivity and thus the errors. The diagonal of the hessian contains the second differential of the cost function for each input parameter. This describes the curvature of the cost function in the direction of each parameter. A smaller curvature means a smaller sensitivity and thus a larger error.

\begin{figure}
  \centering
  \includegraphics[width=10cm]{images/niceExample.png}
  \caption{Electron density profile inferred by \gls{nice} for an instance in time within the \gls{west} tokamak.}
  \label{fig:nice_example}
\end{figure}

\section{Bayesian Inference and Gaussian Process Regression}

This work aims to use Bayesian inference to obtain the electron density profile. Bayes theorem for a physical quantity of interest $q$ is expressed as 

\begin{equation} 
  P(q|D,I) = \frac{P(D|q,I) P(q|I)}{P(D|I)},
  \label{eq:bayesth}
\end{equation}

the posterior $P(q|D,I)$ is the probability density distribution of $q$ given the measured data $D$ and some prior information $I$. The maximum of the posterior is the most probable value of $q$ given the data and prior information. The uncertainty of $q$ can also be obtained from the posterior. The likelihood $P(D|q,I)$ is the probability density function that expresses the probability of the measured data given a fixed value of $q$ and the prior information. The likelihood is described by the experimental error for the data collection. The prior $P(q|I)$ contains information assumed about $q$ before the data is taken. The marginal likelihood or evidence $P(D|I)$ is simply the probability of the data given the prior information only. For posterior computation the marginal likelihood serves as a normalisation factor. Normalisation is often carried out with other means to simplify the posterior computation. Although the marginal likelihood can be used to tune hyperparameters. For example the degree or strength of prior information is uncertain and by finding the strength that maximises the marginal likelihood we find the prior that match's the data the best. Maximising the marginal likelihood to tune the hyperparameters also aids to avoid over-fitting, as the trade off between model complexity and data-fit is automatic via the Occam's razor principle \cite{oscraz}. The marginal likelihood method is powerful although it is important to remember that it is not perfect and does not guarantee to find the hyperparameters that lead to the most accurate posterior.

\gls{gpr} is a form of Bayesian inference where we assume these distributions can be expressed with multivariate Gaussian distributions. When the prior and posterior have the same form then the prior is known as a conjugate prior. This simplifies the inference as it is possible to find a closed form expression of the posterior. Without a closed form expression the posterior must be approximated with sampling techniques. \gls{gpr} is the technique used in this work to infer the electron density profile from interferometry data. First the technique is introduced for a simple regression problem where we wish to fit a curve to a set of points.

A multivariate Gaussian can be used to model a curve and its uncertainty, as illustrated in the violin plot of figure \ref{fig:mvg}. The functional form of a multivariate guassian is

\begin{equation}
  \mathcal{N}(\vec y, \vec{\mu}, \Sigma) = \frac{1}{\sqrt{(2\pi)^{\frac{n}{2}}|\Sigma|}} e^{{-\frac{1}{2}(\vec{y}-\vec{\mu})^T\Sigma^{-1}(\vec{y}-\vec{\mu})}},
  \label{eq:mvg}
  \end{equation}

the mean vector $\vec{\mu}$ holds the $y$ values of the curve at regular intervals along the $x$ axis. The diagonal of the covariance matrix holds the standard deviations of each Gaussian within the multivariate. These represent the errors of the curve. Figure \ref{fig:mvg} shows 10 Gaussians but in practice many are used to ensure a smooth curve. In this way a multivariate gaussian can be used to represent the curve resulting from \gls{gpr}.

\begin{figure}
  \centering
  \includegraphics[width=10cm]{images/mvg.png}
  \caption{Illustrating how many Gaussians can model a line and its uncertainty.}
  \label{fig:mvg}
\end{figure}

The simple regression problem consists of points to which one would like to fit a smooth curve. To solve this using \gls{gpr} one must define the likelihood and prior as multivariate Gaussians. If $m$ measurements are taken the likelihood is

\begin{equation}
  \mathcal{N}(\vec{d}, R\vec{y}, \Sigma_{li}),\hspace{0.5cm} \Sigma_{li} = \vec{\epsilon}I = 
    \begin{vmatrix}
        \epsilon_1 & 0 & \hdots & 0\\
        0 & \epsilon_2 & \hdots & 0\\
        \vdots & \vdots & \ddots & 0 \\
        0 & 0 & 0 &\epsilon_m
    \end{vmatrix},
  \label{eq:likelihood}
\end{equation}

and the prior is

\begin{equation}
  \mathcal{N}(\vec{y}, \vec{0}, K),\hspace{0.5cm} K_{ij} = k(y_i, y_j) = \sigma e^{\frac{y_i - y_j}{2l^2}}.
  \label{eq:prior}
\end{equation}

The goals is to compute the posterior,

\begin{equation}
  \mathcal{N}(\vec{y}, \vec{\mu_{post}}, \Sigma_{post}),
  \label{eq:posterior}
\end{equation}

which represents the best fit given the data. The many gaussians that make up these multivariate gaussians are visualised in figure \ref{fig:gprvis}. 

\begin{figure}[H]
  \centering
  \includegraphics[width=17cm]{images/gprvis.png}
  \caption{A visualisation of the simple \gls{gpr} process.}
  \label{fig:gprvis}
\end{figure}


This visualisation aid is helpful to appreciate the role of the likelihood and prior but it is not always a perfect representation. The likelihood has a diagonal covariance matrix and so the multiple gaussians that make up the likelihood are not dependant on eachother, thus representing them individually provides all the infomation in the likelihood. The prior has a more complex covariance matrix $K$, and figure \ref{fig:gprvis} does not have a complete representation of the priors form. To compute the posterior in figure \ref{fig:gprvis} one must understand the components of the posterior, likelihood, prior and how they can be combined into the closed form equations. Startng with the posterior, equation \ref{eq:posterior}, $\vec{y}$ contains the random variables $y_i$ for regularly spaced positions $x_i$. Given the data the most probable values of $\vec{y_i}$ are in $\vec{\mu_{post}}$ and their uncertinites in the diagonal of $\Sigma_{post}$. In the likelihood equation, \ref{eq:likelihood}, $\vec{d}$ is the data vector containing all the $y$-values from measurements at various $x$ positions. $R$ is known as the responce matrix and ensures that the mean vector of the likelihood $R\vec{y}$ only contains values of $\vec{y}$ at the same $x$ positions the data was recorded at. $R$ is essentially a filter that removes irrelevant $y_i$ values from $\vec{y}$ for the mean vector of the likelyhood. This ensures the mean vector has the same length as the data vector. In the likelihood of figure \ref{fig:gprvis} you can see an example of a given $\vec{y}$ and the mean of each gaussian is the value on that line at the same $x$ position as the data point. $\vec{\epsilon}$ contains the experimental error of the measurements in $\vec{d}$. In the prior, equation \ref{eq:prior}, there are as many gaussians as in the posterior but each of these gaussians have a mean of 0. The $\vec{0}$ symbol represents a vector of zeros the same length as the vector $\vec{y}$ of random variables. To aid visualisation the prior in figure $\ref{fig:gprvis}$ only shows 5 out of the 101 gaussians used. The covariance matrix $K$ is constructed using the kernel $k(x_i,x_j)$. The main role of the amplitude, $\sigma$, in the kernel is to set the prior strength. The length scale, $l$ sets the strength of corelation of neighbouring gaussians. A low length scale means that only gaussians close in $x$ are highly correlated. Gaussians further in x would have a low correlation, meaning they can have a very different mean value. A low length scale allows the fitted curve to have more complexity simililar to a high order polynomial and can lead to overfitting. A high length scale limits the fits ability to curve sharply leading to a simple model, similar to a low order polynolial, leading to underfitting. A very high length scale leads to an almost linear fit, every time. This prior is far from perfect. For instance it is often known that the infered values must be positive, for example you cannot have a negative electron density. Since the prior mean vector is set to $\vec{0}$, a negative value is as likely to be infered as a positive value. Since it is gaussian values close to 0 are more likely to be infered than values far from 0. To mitigate this a high amplitude can be used to lower the prior strength and allow the data in the likelyhod to have more influence on the posterior result. The kernel $k(x_i,x_j)$ in equation \ref{eq:prior} is known as the static exponential square kernel. It is a very commonly used kernel in \gls{gpr} but far from the only choice. The single value of length scale prevents the inference from haveing long smooth regions with few features followd by regions of high variability. This can be an issue when infering H-mode tokamak plasmas that have a sharp drop off in density at the edge. For these situations a non static kernel can be used that alows the lengthscale to be a function of $x$ which can then allow for posteriors of varying complexity. Regardless of the kernel used, deciding the optimal values of its parameters for a problem is not obvious. A common solution is to use the marginal likelyhood of equation \ref{eq:bayesth} as a loss function. The parameters that maxemise it can be found with gradient based methods. This finds the parameters that maximises the probability of the data being measured. Although the marginal likelyhood method is also known for automatically deploying Occam's razor principle which finds a balance between closely fitting the data and having a simple model that accounts for the data's errors to have a more accurate inference \cite{oscraz} \cite{gp4ml}. Essentially maxemising the marginal likelihood avoids overfitting. Although this method is powerful it does not guarentee to produce parameters that lead to the most accurate fit. To get a more acurate fit, bayesian sampleing techniques can be used, although this is more computationally expensive. 

The posteriors mean vector and covariance matrix can be expressed by the closed form expressions

\begin{equation}
\mu_{post}= \vec{\mu}_{pr} + (K^{-1} + R^{\top} \Sigma_{li}^{-1} R)^{-1} R^{\top} \Sigma_{li}^{-1} (\vec{d} - R \vec{\mu}_{pr}),
\end{equation}

\begin{equation}
\Sigma_{post} = \left(R^\top \Sigma_{li}^{-1} R + K^{-1}\right)^{-1},
\end{equation}

these are derived in appendix \ref{append:dervcf}. The main steps include multiplying the functional forms of the prior and likelihood, ignoring all scaling factors, simplifying untill they form a single un normaised multivariate gaussian and then comparing this with the posterior.

The marginal likelihood can be exppressed as

\begin{equation}
\begin{aligned}
 P(\vec d|\vec\epsilon,\theta) &= \int P(\vec{d}|\vec{y},\vec\epsilon)P(\vec{y}|\theta)  \, d\vec y \\
 &= \frac{1}{(2\pi)^{\frac{m}{2}} \sqrt{|\Sigma_{li} + RKR^\top|}} \exp\left[ -\frac{1}{2} (\vec{d} - R\vec{\mu}_{pr})^{\top} (\Sigma_{li} + R K R^{\top})^{-1} (\vec{d} - R\vec{\mu}_{pr}) \right].
\end{aligned}
\end{equation}

The values of the marginal likelihood can become very large and troublesome to compute with standard 64bit float precision. For this reason the logarithm is computed. It is convention when perfoming optimisation to define a loss function to be minimised, thus the negative log marginal likelihood is used. Scaling constants do not affect the minimum value and can be ignored. The negative log marginal likelihood used as a loss function for hyper-parameters is then 

\begin{equation}
-\ln(P(\vec d| \vec \epsilon,\theta)) \propto \ln(|\Sigma_{li}+RKR^\top|) +  (\vec{d} - R\vec{\mu}_{pr})^{\top} (\Sigma_{li} + R K R^{\top})^{-1} (\vec{d} - R\vec{\mu}_{pr}),
\end{equation}

the full derivation of this expression can be found in appendix \ref{append:dervml}.


\section{Interferometry and Polarimetry}

\begin{figure}
  \centering
  \includegraphics[width=10cm]{images/inferfgeo.png}
  \caption{The geometry of interfero-polarimetry lasers at \gls{WEST} \cite{westinterfero}.}
  \label{fig:magfluxsurf}
\end{figure}

Interferometry and polarimetry are two diagnostics obtained from the same hardware. Lasers are fired through the plasma at multiple angles. The geometry at \gls{west} is shown in figure \ref{fig:magfluxsurf}. The refractive index of the plasma is dependant on the electron density. The lasers slow and the phase shift can be measured win interferometry. This phase shift is proportinal the line integrated electron density $n_e$ along the line of sight of the lasers, equation \ref{eq:int_phase} \cite{princPlasDiag}.

\begin{equation} 
  \label{eq:int_phase}
  \Delta\phi = \frac{\lambda e^{2}}{4 \pi \epsilon_0 m_e c^2 } \int n_e dl
\end{equation}

The laser wavelength $\lambda$, is comnined with other common physical constants to assertain the constant of proportionality. \gls{west} have stored the line integreated electron density as raw interferometry data in the \gls{imas} database. This is the data that will be used for this work.

Polarimetry measures the faraday rotation angle of the lasers. The linearly polarised lasers experience a rotation as the circularly polarised components travel through the plasma at different speeds. This is due to the small gyration of the electrons around the margnetic field. The faraday rotation angle is proportinal to the line integrated density of $n_e B_{||}$ along the line of sight of the lasers. Where $B_{||}$ is the magnetic field strenghth paralell to the line of sight. Polarimetry has information about the electron density and this work could be extended to bcome a bayesian integrated analysis which includes this infomation in the inference. Currently only interferometry information is used. Polarimetry can be used in combination with interferometry to gain information about the polodial magnetic field and this is why \gls{nice} uses it to determine the position of the magnetic flux surfaces.

\begin{equation}
  \label{eq:pol_farad}
	\theta_F = \frac{\lambda^2 e^3}{8 \pi^2 c^3 \epsilon_0 m_e^2} \int n_e B_{||}dl
\end{equation}

\section{Gaussian Process Regression for Interferometry}

To infer the electron density profile with interferometry the \gls{gpr} process is altered. $\vec{y}$ becomes $\vec{n_e}$ and the likelihood changes. The $\vec{0}$ prior mean can remain the same, the amplitude $\sigma$ and length-scale $l$ can be re-optimised by maximising the marginal likelihood. The data is now in a different space and thus is the likelyhood. The responce matrix $R$ must be created so that it will transform a profile $\vec{n_e}$ into what would be measured by an error free version of the \gls{west} interferometry system given $\vec{n_e}$ is the true profile. The result of $R \vec{n_e}$ is a vector the same length as the data $\vec{d}$ where each element corresponds to a different interferometry laser or channel. 

The responce matrix computation can be summarised in a few steps. \Gls{nice} provides the magnetic flux at a set of grid points on the tokamak cross section. It also provides the flux at a set of flux surfaces of known normalised radius $\rho$. A simple 1D interpolation can be used to determine the normalised radius at each grid point. Then using $\vec{n_e}$ another 1D interpolation can be done to determine the electron density at each grid point. After the density at any point along a lasers line of sight can be computed using tringuar mesh interpolation. The density at the golden cross in figure \ref{fig:meshtriangle} can be computed as a weighted sum of the three nearest known density points that form the golden triangle.

\begin{figure}
  \centering
  \includegraphics[width=10cm]{images/meshtriangle.png}
  \caption{An example mesh grid to aid visualisation of the triangular mesh grid interpolation used in the responce matrix construction.}
  \label{fig:meshtriangle}
\end{figure}

\begin{equation}
  n_e(l_i) = \lambda_1 n_e(g_1) + \lambda_2 n_e(g_2) + \lambda_3 n_e(g_3)
\end{equation}

The line integrated density can be approximated as a sum of electron densities at many points along the line of sight times the width of their seperation.

\begin{equation}
  \int n_e dl \approx \sum_i n_e(l_i) \Delta l
  \label{eq:neapprox}
\end{equation}

The contribution $w(g_i)$ of each grid point $g_i$ is a sum of all the mesh interpolation coefficiants $\lambda_j$ used on that point.

\begin{equation}
  \int n_e dl \approx \Delta l \sum_i w(g_i) n_e(g_i), \hspace{1cm} w(g_i) = \sum_j \lambda_j  
\end{equation}

Each point can be associated to a nearest flux surface $f_i$ equally spaced in $\rho$. This way the contribution $w(f_i)$ of each flux surface is a sum of the contribution at each of its associated grid points $g_j$.

\begin{equation}
  \int n_e dl \approx \Delta l \sum_i w(f_i) n_e(f_i), \hspace{1cm} f = \sum_j g_j
  \label{eq:neapproxFS}
\end{equation}

All of these steps equate to a simple re-ordering of the origional summation \ref{eq:neapprox} to extract the contribution of each flux surface on the final integrated density value. Equation \ref{eq:neapproxFS} can be computed using a vector product.

\begin{equation}
  \int n_e dl \approx \Delta l \vec{w}^{\top} \vec{n_e}
\end{equation}

The contribution vector applies to one line of sight. The computation for all lines of sight can be performed by placing the $\vec{w}$ vector for each line of sight as a row in the responce matrix $R$. Creating a line integrated density vector $\vec{lid}$. The $\Delta l$ is also multiplied into the matrix to simplify notation.

\begin{equation}
  \vec{lid} \approx R \vec{n_e}
\end{equation}

This responce matrix $R$ can then be used in the closed form expressions \ref{}, to perfrom a 1D electron density profile inference.

Some further alterations to the \gls{gpr} method can be made to further increase the reliability of the inference. These include altering the kernel and adding artifical observations to include prior knowledge. The kernel can be changed to a non static kernel. This allows the length scale to change as a function of $\rho$. 

\begin{equation}
  K_{ij} = k(\rho_i, \rho_j) = \sigma^2 \left( \frac{2l(\rho_i)l(\rho_j)}{l(\rho_i)^2 + l(\rho_j)^2} \right)^{1/2} \exp\left({\frac{(\rho_i - \rho_j)^2}{l(\rho_i)^2+l(\rho_j)^2}}\right)
\end{equation}

The length-scale controls smoothness, model complexity and curvature. If these are free to change for different regions of the plasma then there is a greater range of possabilities for the final inference. Chilenski used a hypernolic tangent function, equation \ref{eq:smoothstep}, to form a smooth step down from a high length scale at the core to low at the edge \cite{chilenski}.

\begin{equation}
  l(\rho) = \frac{l_{core} + l_{edge}}{2} + \frac{l_{core} - l_{edge}}{2} \tanh\left(\frac{\rho-\rho_{center}}{\rho_{width}}\right)
  \label{eq:smoothstep}
\end{equation}

\begin{figure}
  \centering
  \includegraphics[width=10cm]{images/smoothstep.png}
  \caption{Hyperbolic tangent smooth step function for length scale, equation \ref{eq:smoothstep}. Used to capture the drop at the edge of H-mode plasmas \cite{chilenski}.}
  \label{fig:smoothstep}
\end{figure}

The extra freedom at the edge allows the inference to accomodate for a large sudden drop in electron density, which is a common feature for H-mode plasmas. H-mode plasmas are known to have a longer confinement time and thus better fusion performance. \gls{west} does not operate in H-mode, although this method is tested with synthetic data from a simple H-mode simulation.

Put in the cubic spline length scale. 

Traditionally prior information should be included in the prior distribution. Although in practice precision errors can make it more convenient to place prior information into the likelyhood in the form of artificial observations. This practise was also adopted by Chilenski \cite{chilenski}. The density is known to be close to 0 at the edge, ($\rho=1$). It is also known that the density profile is smooth and symetric meaning the gradient of the profile at the core must be close to 0. The point of reflection for the symetry is the core. This information can be included in the data, $\vec{d}$, with an atrificial experimental error determining the strength of the information included in $\vec{\epsilon}$. Other parts of the \gls{gpr} method need to be altered to accomodate the new information. The vector to be infered $\vec{a}$ is not only $\vec{n_e}$ but also includes $n_e(\rho=1)$ and $n_e'(\rho=0)$ concatonated onto the end. This allows the responce matrix alteration to be simple.

\begin{equation}
  R^{alt} =
  \begin{vmatrix}
    R & \cdots & 0 & 0\\
    \vdots & \ddots & 0 & 0\\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1
  \end{vmatrix}
  \label{eq:Ralt}
\end{equation}

The prior covariance matrix must also be altered. The covariance between a gradient and non gradient data point is simply the diferntial of the covariance over the gradient data point. For two gradient data points it is a double differential over each point.

\begin{equation}
  K'_{ij} = k'(\rho_i, \rho_j) = \frac{\partial k'(\rho'_i, \rho_j)}{\partial \rho'_i}  
\end{equation}

\begin{equation}
  K''_{ij} = k'(\rho'_i, \rho'_j) = \frac{\partial k''(\rho'_i, \rho'_j)}{\partial \rho'_i \partial \rho'_j}  
\end{equation}

In this notation $\rho'$ indicates the position of a gradient data point. The alternate kernal is then.

\begin{equation}
  K^{alt} = 
    \begin{matrix}
      K & K'\\
      -K'^{\top} & K''
    \end{matrix}  
\end{equation}

To summarise the electron density profile is important as it plays a key role in determining the energy confinement time and informing real time control systems. With the assumption of magnetic flux surfaces one can express it as a 1D profile, \ref{fig:nice_example}. \gls{nice} is an equilibrium re-construction code that also infers the electron density profile that can be used as a comparison in this thesis. \gls{gpr} is a form of bayesian inference that can be applied to interferometry data to infer the electron density profile. A non static kernel can be used to allow the inference to have model complexity that varies with $\rho$. Prior infomation can be included in the likelihood with artifical observations. In the results section these methods will be deployed on synthetic data. The inference performance can be determined by how closely it fits the ground truth profile. They will also be deployed on real \gls{west} data and the results will be compaired to that obtained by \gls{nice}.

% \begin{figure}
%   \centering
%   \includegraphics[width=\columnwidth]{images/niceExample.png}
%   \hskip 1mm
%   \includegraphics[width=\columnwidth]{images/niceExample.png}
%   \hskip 1mm
%   \vspace{-1cm}
%   \caption{Predictions on completely sampled space.}
%   \label{fig:flowe_grid}
% \end{figure}

% \label{sec:intpol}
% \section{Interfero-polarimetry}
\begin{itemize}
    \item Some basic backround theory of a tokamak as a fusion device. Including the schematics of the magnetics and different systems present. Including the magnetic flux surfaces and what they are, assumptions made. What is $\rho$? Also highlights a little why plasma density profile is important. 
    \item Explain the theory of how NICE did their inference to some degree. ***
    \item Bayes Theorem for inference problems, each distribution involved and why the margianl likelyhood is useful. ***
    \item Explian how a multivariate gaussian can model a curve. ***
    \item Explain Gaussian Process Regression for a generic line fitting problem, including the various distributions constructed and how they are used in the closed form expressions. A little on marginal liklyhood again for parameter optimisation. Mention briefly the alterations to this algorythem that will be made to allow it to be used for interferometry including the responce function change, and virtual observations.
    \item Explain Interferometry and a little on polarimetry, show west laser geometry
    \item Explain the magnetic flux surfaces again a little and how this allows us to make the electron density profile 1D again.
    \item Explain the forward model and how it is computed with the responce matrix.
    \item Explain how the prior edge and core information can be added and the complications of inserting it into the prior.
    \item Explain why a non stationary kernel (smooth step) might be beneficial 
    \item summarise the chapter and explain how it ties into the methodology
\end{itemize}





