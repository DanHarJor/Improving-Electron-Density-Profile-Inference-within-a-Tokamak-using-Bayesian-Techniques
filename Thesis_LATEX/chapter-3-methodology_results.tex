\chapter{Methodology and Results}

%methodology

% Explain the details of the how the theory is implimented, what processing power is required, I used python jupyter notebooks numpy, scipy and pytorch. Explain the key parts of the code in order to compute the kernels, marginal likelyhood and inference. What test were done in order to try and improve the inference. 

% \begin{itemize}
%     \item What format was the data in when it came from west and how did I import it into python
%     \item A bit about the diffent forms the data comes in on the west imas database and which one was chosen. 
%     \item How can the accuracy of the forward model be tested and how should this effect the experimental error.
%     \item How does one select an experimental error, is it important for the inference? 
%     \item What is the non positive definite matrix error, how can it be avoided, what are the implications of this
%     \item How I used meshgrid to compute the kernel in a fast way
%     \item what are the different methods used for finding the minimum of the marginal likelyhood, scipy minimize, pytorch, grid search. How do they work. 
%     \item What distributions did I use to randomly initialize the kernel parameters. Why did I randomly initialise them?
%     \item Why I coded everything with pytorch tensors
%     \item How I optimised the learning rate
%     \item Attempts to use noise
%     \item How did I try and improve the inference.
%     \item summarise the chapter and lead into the results. 
    
% \end{itemize}


% \begin{itemize}
%     \item Make the case as why a 0 mean prior is not always a good Idea, since the marginal likelyhood is not perfect and inferences favour the lower side of NICE, likely due to the 0 prior as when amp is increased the inference will move to NICE but not beyond it. Including prior knowledge when available is always advised.
    
%     \item Make the case that if I set NICE as the true ground truth and create synthetic data then I get the NICE profile from lowering marginal likelyhood. This shows that everything is set up correctly.
    
%     \item Show I am getting a local minimum of the marginal likelihood which is nice like and a lower minimum which is parabolic. This could be a result of the marginal likelihood not being a perfect loss function for problems with little data. This seems to be true for SciPy, PyTorch, static and non-static.
%     \item Talk about the discovered more defined ridge, ask for access to log book to see if this is during a H-mode shot. Explain why it might be physically significant but a full monte carlo bayesian should be carried out to verify this. 
%     \item Show the static grid search

%     \item Results not yet curated
%         \subitem Bayesian inference using monte carlo methods
%         \subitem One size fits all kernel for real time. 
%     \item summarise the chapter and lead into conclusions
% \end{itemize}